{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUjMwMI8Kotk"
   },
   "source": [
    "# <span style=\"color:#0b486b\">  FIT5215: Deep Learning (2023)</span>\n",
    "***\n",
    "*CE/Lecturer:* Dr **Trung Le** | trunglm@monash.edu <br/>\n",
    "*Head Tutor:* Mr **Tuan Nguyen** | tuan.Ng@monash.edu  <br/>\n",
    "<br/>\n",
    "Department of Data Science and AI, Faculty of Information Technology, Monash University, Australia\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_V4popnCKotl"
   },
   "source": [
    "# <span style=\"color:#0b486b\">  Student Information</span>\n",
    "***\n",
    "Surname: **NG**  <br/>\n",
    "Firstname: **YI JIE**    <br/>\n",
    "Student ID: **31158145**    <br/>\n",
    "Email: **yngg0039@student.monash.edu**    <br/>\n",
    "Your tutorial time: **Monday 10:00am to 12:00pm**    <br/>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXkDASf9Kotm"
   },
   "source": [
    "# <span style=\"color:#0b486b\">Deep Neural Networks</span>\n",
    "### Due: <span style=\"color:red\">11:59pm Sunday, 10 September 2023</span>  (Sunday)\n",
    "\n",
    "#### <span style=\"color:red\">Important note:</span> This is an **individual** assignment. It contributes **20%** to your final mark. Read the assignment instruction carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjYl9uH9Kotm"
   },
   "source": [
    "## <span style=\"color:#0b486b\">Instruction</span>\n",
    "\n",
    "This notebook has been prepared for you to complete Assignment 1. The theme of this assignment is about practical knowledge and skills in deep neural networks, including feedforward and convolutional neural networks. Some sections have been partially completed to help you get started. **The total marks for this notebook is 100**.\n",
    "\n",
    "* Before getting started, you should read the entire notebook carefully once to understand what you need to do. <br/>\n",
    "\n",
    "* For each cell marked with **#YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL**, there will be places where you **must** supply your own codes when instructed. <br>\n",
    "\n",
    "This assignment contains **three** parts:\n",
    "\n",
    "* **Part 1**: Questions on theory and knowledge on deep learning **[35 points], 35%**\n",
    "* **Part 2**: Coding assessment on TensorFlow for Deep Neural Networks (DNN) **[25 points], 25%**\n",
    "* **Part 3**: Coding assessment on TensorFlow for Convolution Neural Networks (CNN) **[40 points], 40%**\n",
    "\n",
    "**Hint**: This assignment was essentially designed based on the lectures and tutorials sessions covered from Week 1 to Week 5. You are strongly recommended to go through these contents thoroughly which might help you to complete this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejnZYAlwKotm"
   },
   "source": [
    "## <span style=\"color:#0b486b\">What to submit</span>\n",
    "\n",
    "This assignment is to be completed individually and submitted to Moodle unit site. **By the due date, you are required to submit one  <span style=\"color:red; font-weight:bold\">single zip file, named xxx_assignment01_solution.zip</span> where `xxx` is your student ID, to the corresponding Assignment (Dropbox) in Moodle**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqrLa-wdKotm"
   },
   "source": [
    "***For example, if your student ID is <span style=\"color:red; font-weight:bold\">12356</span>, then gather all of your assignment solution to folder, create a zip file named <span style=\"color:red; font-weight:bold\">123456_assignment01_solution.zip</span> and submit this file.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zmFDQNuKotm"
   },
   "source": [
    "Within this zip folder, you **must** submit the following files:\n",
    "1.\t**Assignment01_solution.ipynb**:  this is your Python notebook solution source file.\n",
    "1.\t**Assignment01_output.html**: this is the output of your Python notebook solution *exported* in html format.\n",
    "1.\tAny **extra files or folder** needed to complete your assignment (e.g., images used in your answers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWCaucwIKotm"
   },
   "source": [
    "Since the notebook is quite big to load and work together, one recommended option is to split solution into three parts and work on them seperately. In that case, replace **Assignment01_solution.ipynb** by three notebooks: **Assignment01_Part1_solution.ipynb**, **Assignment01_Part2_solution.ipynb** and **Assignment01_Part3_solution.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RU12JR7wKotn"
   },
   "source": [
    "**You can run your codes on Google Colab. In this case, you have to make a copy of your Google colab notebook including the traces and progresses of model training before submitting.**\n",
    "\n",
    "You also need to store your trained models to folder <span style=\"color:red; font-weight:bold\">*./models*</span> with recognizable file names (e.g., Part3_Sec3_2_model.h5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-guZ8FXTKotv"
   },
   "source": [
    "## <span style=\"color:#0b486b\">Part 2: Deep Neural Networks (DNN) </span>\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 25 points]<span></div>\n",
    "\n",
    "The first part of this assignment is to demonstrate your basis knowledge in deep learning that you have acquired from the lectures and tutorials materials. Most of the contents in this assignment are drawn from **the tutorials covered from weeks 1 to 4**. Going through these materials before attempting this assignment is highly recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnZ28j19Kotv"
   },
   "source": [
    "In the first part of this assignment, you are going to work with the **FashionMNIST** dataset for *image recognition task*. It has the exact same format as MNIST (70,000 grayscale images of 28 Ã— 28 pixels each with 10 classes), but the images represent fashion items rather than handwritten digits, so each class is more diverse, and the problem is significantly more challenging than MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwtTsZDhKotv"
   },
   "source": [
    "####  <span style=\"color:red\">**Question 2.1**</span>. Load the Fashion MNIST using Keras datasets\n",
    "\n",
    "<div style=\"text-align: right\"> <span style=\"color:red\">[5 points]</span> </div>\n",
    "\n",
    "We first use keras incoporated in TensorFlow 2.x for loading the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dHoIW_FOKotw"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XlsqhjWTKotw"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(31158145)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccFfzQZwKotw"
   },
   "source": [
    "We first use keras datasets in TF 2.x to load Fashion MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gZ5vTMjOKotw"
   },
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full_img, y_train_full), (X_test_img, y_test) = fashion_mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dlZuVLyKotw"
   },
   "source": [
    "The shape of X_train_full_img is $(60000, 28, 28 )$ and that of X_test_img is $(10000, 28, 28)$. We next convert them to matrices of vectors and store in X_train_full and X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvkhDRivKotw",
    "outputId": "755af21b-2143-41f1-e058-90147e667f6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n",
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "num_train = X_train_full_img.shape[0]\n",
    "num_test = X_test_img.shape[0]\n",
    "#Get X_train_full and X_test\n",
    "\n",
    "print(num_train)\n",
    "print(num_test)\n",
    "\n",
    "X_train_full = X_train_full_img\n",
    "X_test = X_test_img\n",
    "\n",
    "#Shuffle X_train_full with the label\n",
    "train_val_idx = np.random.permutation(num_train)\n",
    "\n",
    "X_train_full = X_train_full[train_val_idx]\n",
    "y_train_full = y_train_full[train_val_idx]\n",
    "\n",
    "print(X_train_full.shape, y_train_full.shape)\n",
    "print(X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZDRZ6sFKotw"
   },
   "source": [
    "####  <span style=\"color:red\">**Question 2.2**</span>. Preprocess the dataset and split into training, validation, and testing datasets\n",
    "\n",
    "<div style=\"text-align: right\"> <span style=\"color:red\">[5 points]</span> </div>\n",
    "\n",
    "You need to write the code to address the following requirements:\n",
    "- Use $10 \\%$ of X_train_full for validation and the rest of X_train_full for training. This splits X_train_full and y_train_full into X_train, y_train ($90 \\%$) and X_valid, y_valid ($10 \\%$).\n",
    "- Finally, scale the pixels of X_train, X_valid, and X_test to $[0,1]$) (i.e., $X = X/255.0$).\n",
    "\n",
    "You have now the separate training, validation, and testing sets for training your model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUq8gM_KKotw",
    "outputId": "7790199d-f05e-4fe9-93b0-bb9fb56a1705"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "N = X_train_full.shape[0]\n",
    "i = math.floor(0.9*N)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# reshape\n",
    "X_train_full, X_test = X_train_full.reshape(num_train, -1), X_test.reshape(num_test, -1)\n",
    "\n",
    "# split Train full set to train and valid set\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size = 0.1)\n",
    "X_train, X_valid, X_test = X_train/255.0, X_valid/255.0, X_test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZ9Oq6f7Kotw"
   },
   "source": [
    "####  <span style=\"color:red\">**Question 2.3**</span>. Write code for the feed-forward neural net using TF 2.x\n",
    "\n",
    "<div style=\"text-align: right\"> <span style=\"color:red\">[5 points]</span> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdUBefTbKotw"
   },
   "source": [
    "We now develop a feed-forward neural network with the architecture $784 \\rightarrow 40(ReLU) \\rightarrow 30(ReLU) \\rightarrow 10(softmax)$. You can choose your own way to implement your network and an optimizer of interest. You should train model in $50$ epochs and evaluate the trained model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pzUmJBoXKotw",
    "outputId": "9abe9eb4-7472-4039-cdc7-b33d3975e0e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================\n",
      "n1=40, n2=30, activation function=relu, learning rate=0.01, optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001FEB9B5A430>\n",
      "Epoch 1: train acc=0.8251, train loss=0.7822 | valid acc=0.8160, valid loss= 0.7900\n",
      "Epoch 2: train acc=0.8180, train loss=0.7712 | valid acc=0.8075, valid loss= 0.7831\n",
      "Epoch 3: train acc=0.8260, train loss=0.7436 | valid acc=0.8152, valid loss= 0.7559\n",
      "Epoch 4: train acc=0.8266, train loss=0.7395 | valid acc=0.8153, valid loss= 0.7526\n",
      "Epoch 5: train acc=0.8268, train loss=0.7369 | valid acc=0.8160, valid loss= 0.7504\n",
      "Epoch 6: train acc=0.8264, train loss=0.7357 | valid acc=0.8150, valid loss= 0.7497\n",
      "Epoch 7: train acc=0.8252, train loss=0.7378 | valid acc=0.8142, valid loss= 0.7523\n",
      "Epoch 8: train acc=0.8251, train loss=0.7369 | valid acc=0.8145, valid loss= 0.7514\n",
      "Epoch 9: train acc=0.8238, train loss=0.7402 | valid acc=0.8130, valid loss= 0.7553\n",
      "Epoch 10: train acc=0.8253, train loss=0.7352 | valid acc=0.8140, valid loss= 0.7502\n",
      "Epoch 11: train acc=0.8235, train loss=0.7394 | valid acc=0.8110, valid loss= 0.7550\n",
      "Epoch 12: train acc=0.8266, train loss=0.7304 | valid acc=0.8152, valid loss= 0.7462\n",
      "Epoch 13: train acc=0.8263, train loss=0.7315 | valid acc=0.8145, valid loss= 0.7472\n",
      "Epoch 14: train acc=0.8263, train loss=0.7314 | valid acc=0.8150, valid loss= 0.7470\n",
      "Epoch 15: train acc=0.8278, train loss=0.7253 | valid acc=0.8178, valid loss= 0.7404\n",
      "Epoch 16: train acc=0.8279, train loss=0.7260 | valid acc=0.8172, valid loss= 0.7412\n",
      "Epoch 17: train acc=0.8291, train loss=0.7224 | valid acc=0.8178, valid loss= 0.7374\n",
      "Epoch 18: train acc=0.8279, train loss=0.7246 | valid acc=0.8172, valid loss= 0.7396\n",
      "Epoch 19: train acc=0.8281, train loss=0.7249 | valid acc=0.8165, valid loss= 0.7401\n",
      "Epoch 20: train acc=0.8290, train loss=0.7243 | valid acc=0.8178, valid loss= 0.7398\n",
      "Epoch 21: train acc=0.8286, train loss=0.7249 | valid acc=0.8177, valid loss= 0.7404\n",
      "Epoch 22: train acc=0.8293, train loss=0.7226 | valid acc=0.8170, valid loss= 0.7375\n",
      "Epoch 23: train acc=0.8314, train loss=0.7175 | valid acc=0.8193, valid loss= 0.7322\n",
      "Epoch 24: train acc=0.8289, train loss=0.7230 | valid acc=0.8170, valid loss= 0.7385\n",
      "Epoch 25: train acc=0.8293, train loss=0.7209 | valid acc=0.8175, valid loss= 0.7358\n",
      "Epoch 26: train acc=0.8310, train loss=0.7176 | valid acc=0.8190, valid loss= 0.7325\n",
      "Epoch 27: train acc=0.8309, train loss=0.7166 | valid acc=0.8183, valid loss= 0.7315\n",
      "Epoch 28: train acc=0.8311, train loss=0.7160 | valid acc=0.8193, valid loss= 0.7311\n",
      "Epoch 29: train acc=0.8306, train loss=0.7177 | valid acc=0.8187, valid loss= 0.7321\n",
      "Epoch 30: train acc=0.8311, train loss=0.7159 | valid acc=0.8207, valid loss= 0.7306\n",
      "Epoch 31: train acc=0.8313, train loss=0.7162 | valid acc=0.8198, valid loss= 0.7307\n",
      "Epoch 32: train acc=0.8299, train loss=0.7180 | valid acc=0.8192, valid loss= 0.7327\n",
      "Epoch 33: train acc=0.8310, train loss=0.7145 | valid acc=0.8200, valid loss= 0.7289\n",
      "Epoch 34: train acc=0.8322, train loss=0.7127 | valid acc=0.8210, valid loss= 0.7276\n",
      "Epoch 35: train acc=0.8320, train loss=0.7126 | valid acc=0.8208, valid loss= 0.7270\n",
      "Epoch 36: train acc=0.8315, train loss=0.7125 | valid acc=0.8207, valid loss= 0.7274\n",
      "Epoch 37: train acc=0.8336, train loss=0.7090 | valid acc=0.8232, valid loss= 0.7229\n",
      "Epoch 38: train acc=0.8325, train loss=0.7121 | valid acc=0.8202, valid loss= 0.7265\n",
      "Epoch 39: train acc=0.8320, train loss=0.7124 | valid acc=0.8207, valid loss= 0.7267\n",
      "Epoch 40: train acc=0.8313, train loss=0.7142 | valid acc=0.8200, valid loss= 0.7286\n",
      "Epoch 41: train acc=0.8317, train loss=0.7129 | valid acc=0.8212, valid loss= 0.7275\n",
      "Epoch 42: train acc=0.8338, train loss=0.7077 | valid acc=0.8217, valid loss= 0.7215\n",
      "Epoch 43: train acc=0.8309, train loss=0.7155 | valid acc=0.8192, valid loss= 0.7301\n",
      "Epoch 44: train acc=0.8321, train loss=0.7114 | valid acc=0.8208, valid loss= 0.7254\n",
      "Epoch 45: train acc=0.8320, train loss=0.7119 | valid acc=0.8205, valid loss= 0.7258\n",
      "Epoch 46: train acc=0.8320, train loss=0.7133 | valid acc=0.8225, valid loss= 0.7285\n",
      "Epoch 47: train acc=0.8320, train loss=0.7119 | valid acc=0.8218, valid loss= 0.7265\n",
      "Epoch 48: train acc=0.8338, train loss=0.7086 | valid acc=0.8223, valid loss= 0.7231\n",
      "Epoch 49: train acc=0.8319, train loss=0.7123 | valid acc=0.8205, valid loss= 0.7268\n",
      "Epoch 50: train acc=0.8323, train loss=0.7110 | valid acc=0.8215, valid loss= 0.7253\n",
      "\n",
      "test acc=0.8197, test loss=0.7432 \n"
     ]
    }
   ],
   "source": [
    "#Insert your code here and you can add more cells if necessary\n",
    "from tensorflow import keras\n",
    "\n",
    "class DNN:\n",
    "    def __init__(self, n_classes=10, optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
    "                 batch_size=32, epochs=50, lr=0.01, activation_choice = 'relu', n1 = 40, n2 = 30):\n",
    "        self.n_classes = n_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.optimizer = optimizer\n",
    "        self.lr = lr  # hyper-parameter corresponding to l2 loss\n",
    "\n",
    "        # custom parameter\n",
    "        self.act = activation_choice\n",
    "        self.n1 = n1\n",
    "        self.n2 = n2\n",
    "        self.test_acc = 0\n",
    "\n",
    "        # create a tensorflow dataset for train, test and valid sets\n",
    "        self.train_set = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        self.valid_set = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "        self.test_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "        # batching train, test and valid sets\n",
    "        self.train_set = self.train_set.batch(self.batch_size).prefetch(1)\n",
    "        self.valid_set = self.valid_set.batch(self.batch_size).prefetch(1)\n",
    "        self.test_set = self.test_set.batch(self.batch_size).prefetch(1)\n",
    "        tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "    def build(self):\n",
    "        self.model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(self.n1, activation=self.act),\n",
    "            tf.keras.layers.Dense(self.n2, activation=self.act),\n",
    "            tf.keras.layers.Dense(self.n_classes, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    def compute_loss(self, X, y):  # X is data batch, y is label batch\n",
    "        pred_probs = self.model(X)\n",
    "        l1 = tf.keras.losses.sparse_categorical_crossentropy(y, pred_probs)  # Cross entropy loss\n",
    "        l2 = tf.add_n([tf.nn.l2_loss(w) for w in self.model.trainable_weights])\n",
    "        l2 = tf.expand_dims(l2, axis=-1)\n",
    "        return l1 + self.lr * l2\n",
    "\n",
    "\n",
    "    def compute_grads(self, X, y):\n",
    "        with tf.GradientTape() as g:  # use gradient tape to compute gradients\n",
    "            loss = self.compute_loss(X, y)\n",
    "        grads = g.gradient(loss, self.model.trainable_variables)  # compute gradients w.r.t. all trainable variables\n",
    "        return grads\n",
    "\n",
    "    def train_one_batch(self, X, y):  # train in one batch\n",
    "        grads = self.compute_grads(X, y)\n",
    "        # the gradients will be applied according to optimizer for example SGD, Adam, and etc.\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "    def evaluate(self, tf_dataset=None):\n",
    "        dataset_loss = tf.keras.metrics.Mean()\n",
    "        dataset_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        for X, y in tf_dataset:\n",
    "            loss = self.compute_loss(X, y)\n",
    "            dataset_loss.update_state(loss)\n",
    "            dataset_accuracy.update_state(y, self.model(X, training=False))\n",
    "        return dataset_loss.result(), dataset_accuracy.result()\n",
    "\n",
    "    def train(self):\n",
    "        print(\"===================================================================\")\n",
    "        print(\"n1={}, n2={}, activation function={}, learning rate={}, optimizer={}\".format(self.n1,self.n2,self.act,self.lr,self.optimizer))\n",
    "        for epoch in range(self.epochs):\n",
    "            for X, y in self.train_set:  # use batch_index if you want to display something in iterations\n",
    "                self.train_one_batch(X, y)\n",
    "            train_loss, train_acc = self.evaluate(self.train_set)\n",
    "            valid_loss, valid_acc = self.evaluate(self.valid_set)\n",
    "            print('Epoch {}: train acc={:.4f}, train loss={:.4f} | valid acc={:.4f}, valid loss= {:.4f}'.format(\n",
    "                epoch + 1, train_acc, train_loss, valid_acc, valid_loss))\n",
    "        print()\n",
    "\n",
    "    def test(self):\n",
    "        test_loss, test_acc = self.evaluate(self.test_set)\n",
    "        print('test acc={:.4f}, test loss={:.4f} '.format(test_acc, test_loss))\n",
    "        self.test_acc = test_acc\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "dnn = DNN(optimizer=opt, epochs=50, batch_size=64, lr=0.01, activation_choice = 'relu', n1 = 40, n2 = 30)\n",
    "dnn.build()\n",
    "dnn.train()\n",
    "dnn.test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rm520QaKKotx"
   },
   "source": [
    "####  <span style=\"color:red\">**Question 2.4**</span>. Tuning hyper-parameters with grid search\n",
    "<div style=\"text-align: right\"> <span style=\"color:red\">[5 points]</span> </div>\n",
    "\n",
    "Assume that you need to tune the number of neurons on the first and second hidden layers $n_1 \\in \\{20, 40\\}$, $n_2 \\in \\{20, 40\\}$  and the used activation function  $act \\in \\{sigmoid, tanh, relu\\}$. The network has the architecture pattern $784 \\rightarrow n_1 (act) \\rightarrow n_2(act) \\rightarrow 10(softmax)$ where $n_1, n_2$, and $act$ are in their grides. Write the code to tune the hyper-parameters $n_1, n_2$, and $act$. Note that you can freely choose the optimizer and learning rate of interest for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sp5RyJ5KKotx",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================\n",
      "n1=20, n2=20, activation function=relu, learning rate=0.001, optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001FEC4A048E0>\n",
      "Epoch 1: train acc=0.8113, train loss=0.6024 | valid acc=0.7988, valid loss= 0.6256\n",
      "Epoch 2: train acc=0.8290, train loss=0.5487 | valid acc=0.8133, valid loss= 0.5760\n",
      "Epoch 3: train acc=0.8404, train loss=0.5105 | valid acc=0.8215, valid loss= 0.5403\n",
      "Epoch 4: train acc=0.8450, train loss=0.4995 | valid acc=0.8260, valid loss= 0.5310\n",
      "Epoch 5: train acc=0.8507, train loss=0.4840 | valid acc=0.8333, valid loss= 0.5154\n",
      "Epoch 6: train acc=0.8540, train loss=0.4754 | valid acc=0.8353, valid loss= 0.5094\n",
      "Epoch 7: train acc=0.8543, train loss=0.4717 | valid acc=0.8360, valid loss= 0.5095\n",
      "Epoch 8: train acc=0.8587, train loss=0.4598 | valid acc=0.8412, valid loss= 0.4987\n",
      "Epoch 9: train acc=0.8549, train loss=0.4682 | valid acc=0.8400, valid loss= 0.5072\n",
      "Epoch 10: train acc=0.8573, train loss=0.4639 | valid acc=0.8412, valid loss= 0.5035\n",
      "Epoch 11: train acc=0.8595, train loss=0.4596 | valid acc=0.8408, valid loss= 0.5015\n",
      "Epoch 12: train acc=0.8599, train loss=0.4597 | valid acc=0.8418, valid loss= 0.5021\n",
      "Epoch 13: train acc=0.8620, train loss=0.4549 | valid acc=0.8408, valid loss= 0.4987\n",
      "Epoch 14: train acc=0.8621, train loss=0.4549 | valid acc=0.8435, valid loss= 0.4993\n",
      "Epoch 15: train acc=0.8654, train loss=0.4460 | valid acc=0.8467, valid loss= 0.4894\n",
      "Epoch 16: train acc=0.8654, train loss=0.4430 | valid acc=0.8478, valid loss= 0.4859\n",
      "Epoch 17: train acc=0.8651, train loss=0.4440 | valid acc=0.8475, valid loss= 0.4865\n",
      "Epoch 18: train acc=0.8663, train loss=0.4423 | valid acc=0.8488, valid loss= 0.4856\n",
      "Epoch 19: train acc=0.8655, train loss=0.4437 | valid acc=0.8470, valid loss= 0.4877\n",
      "Epoch 20: train acc=0.8661, train loss=0.4434 | valid acc=0.8468, valid loss= 0.4882\n",
      "\n",
      "test acc=0.8445, test loss=0.5114 \n",
      "\n",
      "Parameter ['relu', 20, 20] (activation, n1 ,n2) has current best performance of test accuracy 0.8445\n",
      "===================================================================\n",
      "n1=20, n2=20, activation function=sigmoid, learning rate=0.001, optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001FEC4A048E0>\n",
      "Epoch 1: train acc=0.8071, train loss=0.7618 | valid acc=0.7993, valid loss= 0.7740\n",
      "Epoch 2: train acc=0.8294, train loss=0.7063 | valid acc=0.8218, valid loss= 0.7221\n",
      "Epoch 3: train acc=0.8379, train loss=0.6854 | valid acc=0.8290, valid loss= 0.7032\n",
      "Epoch 4: train acc=0.8430, train loss=0.6744 | valid acc=0.8328, valid loss= 0.6932\n",
      "Epoch 5: train acc=0.8464, train loss=0.6677 | valid acc=0.8368, valid loss= 0.6870\n",
      "Epoch 6: train acc=0.8485, train loss=0.6631 | valid acc=0.8375, valid loss= 0.6826\n",
      "Epoch 7: train acc=0.8503, train loss=0.6596 | valid acc=0.8388, valid loss= 0.6792\n",
      "Epoch 8: train acc=0.8518, train loss=0.6569 | valid acc=0.8388, valid loss= 0.6765\n",
      "Epoch 9: train acc=0.8529, train loss=0.6546 | valid acc=0.8400, valid loss= 0.6743\n",
      "Epoch 10: train acc=0.8537, train loss=0.6527 | valid acc=0.8408, valid loss= 0.6725\n",
      "Epoch 11: train acc=0.8544, train loss=0.6510 | valid acc=0.8413, valid loss= 0.6710\n",
      "Epoch 12: train acc=0.8547, train loss=0.6496 | valid acc=0.8412, valid loss= 0.6696\n",
      "Epoch 13: train acc=0.8550, train loss=0.6484 | valid acc=0.8412, valid loss= 0.6685\n",
      "Epoch 14: train acc=0.8554, train loss=0.6473 | valid acc=0.8425, valid loss= 0.6675\n",
      "Epoch 15: train acc=0.8558, train loss=0.6463 | valid acc=0.8432, valid loss= 0.6667\n",
      "Epoch 16: train acc=0.8562, train loss=0.6454 | valid acc=0.8433, valid loss= 0.6659\n",
      "Epoch 17: train acc=0.8564, train loss=0.6446 | valid acc=0.8425, valid loss= 0.6652\n",
      "Epoch 18: train acc=0.8570, train loss=0.6438 | valid acc=0.8423, valid loss= 0.6646\n",
      "Epoch 19: train acc=0.8575, train loss=0.6432 | valid acc=0.8423, valid loss= 0.6640\n",
      "Epoch 20: train acc=0.8579, train loss=0.6425 | valid acc=0.8432, valid loss= 0.6635\n",
      "\n",
      "test acc=0.8388, test loss=0.6829 \n",
      "\n",
      "Parameter ['relu', 20, 20] (activation, n1 ,n2) has current best performance of test accuracy 0.8445\n",
      "===================================================================\n",
      "n1=20, n2=20, activation function=tanh, learning rate=0.001, optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001FEC4A048E0>\n",
      "Epoch 1: train acc=0.8424, train loss=0.5130 | valid acc=0.8280, valid loss= 0.5338\n",
      "Epoch 2: train acc=0.8557, train loss=0.4821 | valid acc=0.8398, valid loss= 0.5087\n",
      "Epoch 3: train acc=0.8631, train loss=0.4661 | valid acc=0.8463, valid loss= 0.4953\n",
      "Epoch 4: train acc=0.8671, train loss=0.4560 | valid acc=0.8503, valid loss= 0.4873\n",
      "Epoch 5: train acc=0.8703, train loss=0.4485 | valid acc=0.8530, valid loss= 0.4814\n",
      "Epoch 6: train acc=0.8717, train loss=0.4443 | valid acc=0.8555, valid loss= 0.4784\n",
      "Epoch 7: train acc=0.8733, train loss=0.4420 | valid acc=0.8558, valid loss= 0.4772\n",
      "Epoch 8: train acc=0.8743, train loss=0.4403 | valid acc=0.8575, valid loss= 0.4767\n",
      "Epoch 9: train acc=0.8748, train loss=0.4389 | valid acc=0.8583, valid loss= 0.4766\n",
      "Epoch 10: train acc=0.8748, train loss=0.4384 | valid acc=0.8578, valid loss= 0.4772\n",
      "Epoch 11: train acc=0.8751, train loss=0.4384 | valid acc=0.8577, valid loss= 0.4783\n",
      "Epoch 12: train acc=0.8748, train loss=0.4388 | valid acc=0.8587, valid loss= 0.4795\n",
      "Epoch 13: train acc=0.8747, train loss=0.4390 | valid acc=0.8585, valid loss= 0.4806\n",
      "Epoch 14: train acc=0.8746, train loss=0.4391 | valid acc=0.8595, valid loss= 0.4813\n",
      "Epoch 15: train acc=0.8748, train loss=0.4391 | valid acc=0.8602, valid loss= 0.4818\n",
      "Epoch 16: train acc=0.8754, train loss=0.4390 | valid acc=0.8603, valid loss= 0.4821\n",
      "Epoch 17: train acc=0.8755, train loss=0.4387 | valid acc=0.8603, valid loss= 0.4823\n",
      "Epoch 18: train acc=0.8759, train loss=0.4385 | valid acc=0.8593, valid loss= 0.4824\n",
      "Epoch 19: train acc=0.8760, train loss=0.4382 | valid acc=0.8587, valid loss= 0.4825\n",
      "Epoch 20: train acc=0.8758, train loss=0.4380 | valid acc=0.8587, valid loss= 0.4826\n",
      "\n",
      "test acc=0.8529, test loss=0.5061 \n",
      "\n",
      "Parameter ['tanh', 20, 20] (activation, n1 ,n2) has current best performance of test accuracy 0.8529\n",
      "===================================================================\n",
      "n1=20, n2=40, activation function=relu, learning rate=0.001, optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001FEC4A048E0>\n",
      "Epoch 1: train acc=0.8309, train loss=0.5175 | valid acc=0.8150, valid loss= 0.5384\n",
      "Epoch 2: train acc=0.8463, train loss=0.4837 | valid acc=0.8315, valid loss= 0.5085\n",
      "Epoch 3: train acc=0.8507, train loss=0.4691 | valid acc=0.8355, valid loss= 0.4971\n",
      "Epoch 4: train acc=0.8543, train loss=0.4586 | valid acc=0.8382, valid loss= 0.4886\n",
      "Epoch 5: train acc=0.8586, train loss=0.4510 | valid acc=0.8423, valid loss= 0.4825\n",
      "Epoch 6: train acc=0.8624, train loss=0.4460 | valid acc=0.8438, valid loss= 0.4801\n",
      "Epoch 7: train acc=0.8639, train loss=0.4398 | valid acc=0.8460, valid loss= 0.4760\n",
      "Epoch 8: train acc=0.8641, train loss=0.4395 | valid acc=0.8480, valid loss= 0.4764\n",
      "Epoch 9: train acc=0.8640, train loss=0.4396 | valid acc=0.8478, valid loss= 0.4784\n",
      "Epoch 10: train acc=0.8634, train loss=0.4455 | valid acc=0.8440, valid loss= 0.4861\n",
      "Epoch 11: train acc=0.8654, train loss=0.4408 | valid acc=0.8473, valid loss= 0.4822\n",
      "Epoch 12: train acc=0.8651, train loss=0.4423 | valid acc=0.8467, valid loss= 0.4832\n",
      "Epoch 13: train acc=0.8676, train loss=0.4356 | valid acc=0.8500, valid loss= 0.4772\n",
      "Epoch 14: train acc=0.8691, train loss=0.4349 | valid acc=0.8500, valid loss= 0.4783\n",
      "Epoch 15: train acc=0.8705, train loss=0.4309 | valid acc=0.8513, valid loss= 0.4741\n",
      "Epoch 16: train acc=0.8719, train loss=0.4264 | valid acc=0.8517, valid loss= 0.4715\n",
      "Epoch 17: train acc=0.8715, train loss=0.4296 | valid acc=0.8517, valid loss= 0.4749\n",
      "Epoch 18: train acc=0.8728, train loss=0.4272 | valid acc=0.8530, valid loss= 0.4730\n",
      "Epoch 19: train acc=0.8741, train loss=0.4247 | valid acc=0.8540, valid loss= 0.4705\n",
      "Epoch 20: train acc=0.8736, train loss=0.4269 | valid acc=0.8538, valid loss= 0.4740\n",
      "\n",
      "test acc=0.8495, test loss=0.4902 \n",
      "\n",
      "Parameter ['tanh', 20, 20] (activation, n1 ,n2) has current best performance of test accuracy 0.8529\n",
      "===================================================================\n",
      "n1=20, n2=40, activation function=sigmoid, learning rate=0.001, optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001FEC4A048E0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train acc=0.8180, train loss=0.7190 | valid acc=0.8118, valid loss= 0.7303\n",
      "Epoch 2: train acc=0.8360, train loss=0.6746 | valid acc=0.8280, valid loss= 0.6884\n",
      "Epoch 3: train acc=0.8431, train loss=0.6585 | valid acc=0.8347, valid loss= 0.6736\n",
      "Epoch 4: train acc=0.8478, train loss=0.6497 | valid acc=0.8375, valid loss= 0.6661\n",
      "Epoch 5: train acc=0.8504, train loss=0.6441 | valid acc=0.8403, valid loss= 0.6619\n",
      "Epoch 6: train acc=0.8525, train loss=0.6403 | valid acc=0.8428, valid loss= 0.6593\n",
      "Epoch 7: train acc=0.8543, train loss=0.6375 | valid acc=0.8415, valid loss= 0.6573\n",
      "Epoch 8: train acc=0.8556, train loss=0.6353 | valid acc=0.8427, valid loss= 0.6557\n",
      "Epoch 9: train acc=0.8565, train loss=0.6336 | valid acc=0.8440, valid loss= 0.6545\n",
      "Epoch 10: train acc=0.8574, train loss=0.6322 | valid acc=0.8438, valid loss= 0.6535\n",
      "Epoch 11: train acc=0.8581, train loss=0.6312 | valid acc=0.8442, valid loss= 0.6526\n",
      "Epoch 12: train acc=0.8588, train loss=0.6303 | valid acc=0.8442, valid loss= 0.6519\n",
      "Epoch 13: train acc=0.8592, train loss=0.6295 | valid acc=0.8453, valid loss= 0.6513\n",
      "Epoch 14: train acc=0.8592, train loss=0.6289 | valid acc=0.8457, valid loss= 0.6508\n",
      "Epoch 15: train acc=0.8594, train loss=0.6283 | valid acc=0.8453, valid loss= 0.6503\n",
      "Epoch 16: train acc=0.8596, train loss=0.6278 | valid acc=0.8448, valid loss= 0.6499\n",
      "Epoch 17: train acc=0.8597, train loss=0.6274 | valid acc=0.8448, valid loss= 0.6495\n",
      "Epoch 18: train acc=0.8600, train loss=0.6270 | valid acc=0.8452, valid loss= 0.6491\n",
      "Epoch 19: train acc=0.8602, train loss=0.6266 | valid acc=0.8458, valid loss= 0.6488\n",
      "Epoch 20: train acc=0.8604, train loss=0.6262 | valid acc=0.8465, valid loss= 0.6485\n",
      "\n",
      "test acc=0.8452, test loss=0.6657 \n",
      "\n",
      "Parameter ['tanh', 20, 20] (activation, n1 ,n2) has current best performance of test accuracy 0.8529\n",
      "===================================================================\n",
      "n1=20, n2=40, activation function=tanh, learning rate=0.001, optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001FEC4A048E0>\n",
      "Epoch 1: train acc=0.8431, train loss=0.5055 | valid acc=0.8312, valid loss= 0.5236\n",
      "Epoch 2: train acc=0.8600, train loss=0.4630 | valid acc=0.8477, valid loss= 0.4861\n",
      "Epoch 3: train acc=0.8691, train loss=0.4417 | valid acc=0.8555, valid loss= 0.4688\n",
      "Epoch 4: train acc=0.8741, train loss=0.4298 | valid acc=0.8570, valid loss= 0.4605\n",
      "Epoch 5: train acc=0.8773, train loss=0.4230 | valid acc=0.8608, valid loss= 0.4564\n",
      "Epoch 6: train acc=0.8790, train loss=0.4185 | valid acc=0.8618, valid loss= 0.4540\n",
      "Epoch 7: train acc=0.8806, train loss=0.4151 | valid acc=0.8623, valid loss= 0.4522\n",
      "Epoch 8: train acc=0.8816, train loss=0.4128 | valid acc=0.8627, valid loss= 0.4511\n",
      "Epoch 9: train acc=0.8823, train loss=0.4114 | valid acc=0.8640, valid loss= 0.4506\n",
      "Epoch 10: train acc=0.8825, train loss=0.4105 | valid acc=0.8642, valid loss= 0.4507\n",
      "Epoch 11: train acc=0.8829, train loss=0.4102 | valid acc=0.8633, valid loss= 0.4512\n",
      "Epoch 12: train acc=0.8828, train loss=0.4102 | valid acc=0.8630, valid loss= 0.4521\n",
      "Epoch 13: train acc=0.8827, train loss=0.4106 | valid acc=0.8630, valid loss= 0.4533\n",
      "Epoch 14: train acc=0.8827, train loss=0.4112 | valid acc=0.8618, valid loss= 0.4546\n",
      "Epoch 15: train acc=0.8829, train loss=0.4116 | valid acc=0.8608, valid loss= 0.4557\n",
      "Epoch 16: train acc=0.8829, train loss=0.4121 | valid acc=0.8602, valid loss= 0.4566\n",
      "Epoch 17: train acc=0.8827, train loss=0.4126 | valid acc=0.8605, valid loss= 0.4576\n",
      "Epoch 18: train acc=0.8828, train loss=0.4132 | valid acc=0.8600, valid loss= 0.4585\n",
      "Epoch 19: train acc=0.8828, train loss=0.4137 | valid acc=0.8595, valid loss= 0.4594\n",
      "Epoch 20: train acc=0.8826, train loss=0.4142 | valid acc=0.8595, valid loss= 0.4603\n",
      "\n",
      "test acc=0.8569, test loss=0.4865 \n",
      "\n",
      "Parameter ['tanh', 20, 40] (activation, n1 ,n2) has current best performance of test accuracy 0.8569\n",
      "===================================================================\n",
      "n1=40, n2=20, activation function=relu, learning rate=0.001, optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001FEC4A048E0>\n",
      "Epoch 1: train acc=0.8389, train loss=0.5196 | valid acc=0.8220, valid loss= 0.5405\n",
      "Epoch 2: train acc=0.8548, train loss=0.4800 | valid acc=0.8365, valid loss= 0.5048\n",
      "Epoch 3: train acc=0.8571, train loss=0.4736 | valid acc=0.8390, valid loss= 0.5016\n",
      "Epoch 4: train acc=0.8620, train loss=0.4578 | valid acc=0.8458, valid loss= 0.4879\n",
      "Epoch 5: train acc=0.8639, train loss=0.4531 | valid acc=0.8482, valid loss= 0.4851\n",
      "Epoch 6: train acc=0.8691, train loss=0.4399 | valid acc=0.8543, valid loss= 0.4712\n",
      "Epoch 7: train acc=0.8689, train loss=0.4396 | valid acc=0.8538, valid loss= 0.4731\n",
      "Epoch 8: train acc=0.8714, train loss=0.4326 | valid acc=0.8562, valid loss= 0.4669\n",
      "Epoch 9: train acc=0.8684, train loss=0.4401 | valid acc=0.8545, valid loss= 0.4758\n",
      "Epoch 10: train acc=0.8725, train loss=0.4288 | valid acc=0.8560, valid loss= 0.4670\n",
      "Epoch 11: train acc=0.8745, train loss=0.4243 | valid acc=0.8583, valid loss= 0.4612\n",
      "Epoch 12: train acc=0.8721, train loss=0.4312 | valid acc=0.8577, valid loss= 0.4706\n",
      "Epoch 13: train acc=0.8721, train loss=0.4339 | valid acc=0.8562, valid loss= 0.4749\n",
      "Epoch 14: train acc=0.8741, train loss=0.4274 | valid acc=0.8580, valid loss= 0.4685\n",
      "Epoch 15: train acc=0.8764, train loss=0.4206 | valid acc=0.8597, valid loss= 0.4618\n",
      "Epoch 16: train acc=0.8720, train loss=0.4350 | valid acc=0.8543, valid loss= 0.4782\n",
      "Epoch 17: train acc=0.8723, train loss=0.4327 | valid acc=0.8542, valid loss= 0.4770\n",
      "Epoch 18: train acc=0.8761, train loss=0.4224 | valid acc=0.8592, valid loss= 0.4668\n",
      "Epoch 19: train acc=0.8736, train loss=0.4319 | valid acc=0.8557, valid loss= 0.4771\n",
      "Epoch 20: train acc=0.8737, train loss=0.4320 | valid acc=0.8558, valid loss= 0.4770\n",
      "\n",
      "test acc=0.8500, test loss=0.5035 \n",
      "\n",
      "Parameter ['tanh', 20, 40] (activation, n1 ,n2) has current best performance of test accuracy 0.8569\n",
      "===================================================================\n",
      "n1=40, n2=20, activation function=sigmoid, learning rate=0.001, optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001FEC4A048E0>\n",
      "Epoch 1: train acc=0.8183, train loss=0.7100 | valid acc=0.8065, valid loss= 0.7227\n",
      "Epoch 2: train acc=0.8349, train loss=0.6729 | valid acc=0.8207, valid loss= 0.6880\n",
      "Epoch 3: train acc=0.8423, train loss=0.6586 | valid acc=0.8285, valid loss= 0.6750\n",
      "Epoch 4: train acc=0.8462, train loss=0.6507 | valid acc=0.8300, valid loss= 0.6681\n",
      "Epoch 5: train acc=0.8485, train loss=0.6456 | valid acc=0.8340, valid loss= 0.6636\n",
      "Epoch 6: train acc=0.8505, train loss=0.6419 | valid acc=0.8367, valid loss= 0.6604\n",
      "Epoch 7: train acc=0.8519, train loss=0.6391 | valid acc=0.8388, valid loss= 0.6579\n",
      "Epoch 8: train acc=0.8532, train loss=0.6368 | valid acc=0.8405, valid loss= 0.6559\n",
      "Epoch 9: train acc=0.8545, train loss=0.6350 | valid acc=0.8408, valid loss= 0.6543\n",
      "Epoch 10: train acc=0.8555, train loss=0.6334 | valid acc=0.8415, valid loss= 0.6530\n",
      "Epoch 11: train acc=0.8559, train loss=0.6322 | valid acc=0.8425, valid loss= 0.6519\n",
      "Epoch 12: train acc=0.8563, train loss=0.6311 | valid acc=0.8427, valid loss= 0.6511\n",
      "Epoch 13: train acc=0.8566, train loss=0.6302 | valid acc=0.8430, valid loss= 0.6504\n",
      "Epoch 14: train acc=0.8568, train loss=0.6295 | valid acc=0.8437, valid loss= 0.6499\n",
      "Epoch 15: train acc=0.8574, train loss=0.6288 | valid acc=0.8432, valid loss= 0.6494\n",
      "Epoch 16: train acc=0.8577, train loss=0.6283 | valid acc=0.8438, valid loss= 0.6490\n",
      "Epoch 17: train acc=0.8581, train loss=0.6278 | valid acc=0.8445, valid loss= 0.6487\n",
      "Epoch 18: train acc=0.8583, train loss=0.6274 | valid acc=0.8448, valid loss= 0.6484\n",
      "Epoch 19: train acc=0.8584, train loss=0.6270 | valid acc=0.8450, valid loss= 0.6481\n",
      "Epoch 20: train acc=0.8587, train loss=0.6267 | valid acc=0.8448, valid loss= 0.6479\n",
      "\n",
      "test acc=0.8429, test loss=0.6654 \n",
      "\n",
      "Parameter ['tanh', 20, 40] (activation, n1 ,n2) has current best performance of test accuracy 0.8569\n",
      "===================================================================\n",
      "n1=40, n2=20, activation function=tanh, learning rate=0.001, optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001FEC4A048E0>\n",
      "Epoch 1: train acc=0.8510, train loss=0.5015 | valid acc=0.8420, valid loss= 0.5185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train acc=0.8617, train loss=0.4743 | valid acc=0.8483, valid loss= 0.4975\n",
      "Epoch 3: train acc=0.8683, train loss=0.4582 | valid acc=0.8528, valid loss= 0.4860\n",
      "Epoch 4: train acc=0.8722, train loss=0.4491 | valid acc=0.8555, valid loss= 0.4805\n",
      "Epoch 5: train acc=0.8741, train loss=0.4454 | valid acc=0.8572, valid loss= 0.4786\n",
      "Epoch 6: train acc=0.8743, train loss=0.4425 | valid acc=0.8567, valid loss= 0.4773\n",
      "Epoch 7: train acc=0.8753, train loss=0.4412 | valid acc=0.8575, valid loss= 0.4772\n",
      "Epoch 8: train acc=0.8751, train loss=0.4403 | valid acc=0.8552, valid loss= 0.4773\n",
      "Epoch 9: train acc=0.8750, train loss=0.4400 | valid acc=0.8538, valid loss= 0.4777\n",
      "Epoch 10: train acc=0.8753, train loss=0.4397 | valid acc=0.8543, valid loss= 0.4783\n",
      "Epoch 11: train acc=0.8748, train loss=0.4423 | valid acc=0.8520, valid loss= 0.4812\n",
      "Epoch 12: train acc=0.8743, train loss=0.4436 | valid acc=0.8532, valid loss= 0.4824\n",
      "Epoch 13: train acc=0.8741, train loss=0.4453 | valid acc=0.8535, valid loss= 0.4850\n",
      "Epoch 14: train acc=0.8743, train loss=0.4451 | valid acc=0.8537, valid loss= 0.4851\n",
      "Epoch 15: train acc=0.8744, train loss=0.4459 | valid acc=0.8535, valid loss= 0.4869\n",
      "Epoch 16: train acc=0.8742, train loss=0.4465 | valid acc=0.8527, valid loss= 0.4882\n",
      "Epoch 17: train acc=0.8740, train loss=0.4476 | valid acc=0.8522, valid loss= 0.4904\n",
      "Epoch 18: train acc=0.8736, train loss=0.4490 | valid acc=0.8515, valid loss= 0.4927\n",
      "Epoch 19: train acc=0.8728, train loss=0.4504 | valid acc=0.8510, valid loss= 0.4949\n",
      "Epoch 20: train acc=0.8725, train loss=0.4514 | valid acc=0.8498, valid loss= 0.4965\n",
      "\n",
      "test acc=0.8477, test loss=0.5186 \n",
      "\n",
      "Parameter ['tanh', 20, 40] (activation, n1 ,n2) has current best performance of test accuracy 0.8569\n",
      "===================================================================\n",
      "n1=40, n2=40, activation function=relu, learning rate=0.001, optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001FEC4A048E0>\n",
      "Epoch 1: train acc=0.8280, train loss=0.5346 | valid acc=0.8148, valid loss= 0.5481\n",
      "Epoch 2: train acc=0.8484, train loss=0.4804 | valid acc=0.8318, valid loss= 0.4986\n",
      "Epoch 3: train acc=0.8551, train loss=0.4671 | valid acc=0.8360, valid loss= 0.4901\n",
      "Epoch 4: train acc=0.8599, train loss=0.4553 | valid acc=0.8412, valid loss= 0.4821\n",
      "Epoch 5: train acc=0.8634, train loss=0.4465 | valid acc=0.8448, valid loss= 0.4746\n",
      "Epoch 6: train acc=0.8677, train loss=0.4361 | valid acc=0.8478, valid loss= 0.4666\n",
      "Epoch 7: train acc=0.8694, train loss=0.4333 | valid acc=0.8517, valid loss= 0.4642\n",
      "Epoch 8: train acc=0.8696, train loss=0.4334 | valid acc=0.8513, valid loss= 0.4664\n",
      "Epoch 9: train acc=0.8705, train loss=0.4310 | valid acc=0.8530, valid loss= 0.4643\n",
      "Epoch 10: train acc=0.8715, train loss=0.4295 | valid acc=0.8537, valid loss= 0.4632\n",
      "Epoch 11: train acc=0.8712, train loss=0.4310 | valid acc=0.8550, valid loss= 0.4647\n",
      "Epoch 12: train acc=0.8728, train loss=0.4253 | valid acc=0.8568, valid loss= 0.4605\n",
      "Epoch 13: train acc=0.8728, train loss=0.4246 | valid acc=0.8560, valid loss= 0.4594\n",
      "Epoch 14: train acc=0.8751, train loss=0.4190 | valid acc=0.8598, valid loss= 0.4553\n",
      "Epoch 15: train acc=0.8789, train loss=0.4117 | valid acc=0.8608, valid loss= 0.4492\n",
      "Epoch 16: train acc=0.8769, train loss=0.4173 | valid acc=0.8602, valid loss= 0.4550\n",
      "Epoch 17: train acc=0.8805, train loss=0.4091 | valid acc=0.8625, valid loss= 0.4458\n",
      "Epoch 18: train acc=0.8806, train loss=0.4084 | valid acc=0.8630, valid loss= 0.4454\n",
      "Epoch 19: train acc=0.8834, train loss=0.4039 | valid acc=0.8648, valid loss= 0.4419\n",
      "Epoch 20: train acc=0.8814, train loss=0.4079 | valid acc=0.8618, valid loss= 0.4457\n",
      "\n",
      "test acc=0.8597, test loss=0.4767 \n",
      "\n",
      "Parameter ['relu', 40, 40] (activation, n1 ,n2) has current best performance of test accuracy 0.8597\n",
      "===================================================================\n",
      "n1=40, n2=40, activation function=sigmoid, learning rate=0.001, optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001FEC4A048E0>\n",
      "Epoch 1: train acc=0.8249, train loss=0.6916 | valid acc=0.8153, valid loss= 0.7064\n",
      "Epoch 2: train acc=0.8364, train loss=0.6606 | valid acc=0.8252, valid loss= 0.6777\n",
      "Epoch 3: train acc=0.8410, train loss=0.6490 | valid acc=0.8290, valid loss= 0.6673\n",
      "Epoch 4: train acc=0.8433, train loss=0.6430 | valid acc=0.8315, valid loss= 0.6620\n",
      "Epoch 5: train acc=0.8449, train loss=0.6391 | valid acc=0.8340, valid loss= 0.6586\n",
      "Epoch 6: train acc=0.8458, train loss=0.6364 | valid acc=0.8350, valid loss= 0.6562\n",
      "Epoch 7: train acc=0.8471, train loss=0.6345 | valid acc=0.8348, valid loss= 0.6544\n",
      "Epoch 8: train acc=0.8479, train loss=0.6330 | valid acc=0.8343, valid loss= 0.6531\n",
      "Epoch 9: train acc=0.8487, train loss=0.6319 | valid acc=0.8353, valid loss= 0.6521\n",
      "Epoch 10: train acc=0.8495, train loss=0.6309 | valid acc=0.8363, valid loss= 0.6513\n",
      "Epoch 11: train acc=0.8502, train loss=0.6302 | valid acc=0.8367, valid loss= 0.6505\n",
      "Epoch 12: train acc=0.8507, train loss=0.6295 | valid acc=0.8362, valid loss= 0.6499\n",
      "Epoch 13: train acc=0.8509, train loss=0.6290 | valid acc=0.8362, valid loss= 0.6494\n",
      "Epoch 14: train acc=0.8514, train loss=0.6285 | valid acc=0.8358, valid loss= 0.6490\n",
      "Epoch 15: train acc=0.8517, train loss=0.6280 | valid acc=0.8363, valid loss= 0.6486\n",
      "Epoch 16: train acc=0.8517, train loss=0.6276 | valid acc=0.8362, valid loss= 0.6483\n",
      "Epoch 17: train acc=0.8519, train loss=0.6273 | valid acc=0.8360, valid loss= 0.6480\n",
      "Epoch 18: train acc=0.8520, train loss=0.6270 | valid acc=0.8360, valid loss= 0.6477\n",
      "Epoch 19: train acc=0.8524, train loss=0.6267 | valid acc=0.8365, valid loss= 0.6475\n",
      "Epoch 20: train acc=0.8526, train loss=0.6264 | valid acc=0.8368, valid loss= 0.6473\n",
      "\n",
      "test acc=0.8372, test loss=0.6664 \n",
      "\n",
      "Parameter ['relu', 40, 40] (activation, n1 ,n2) has current best performance of test accuracy 0.8597\n",
      "===================================================================\n",
      "n1=40, n2=40, activation function=tanh, learning rate=0.001, optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001FEC4A048E0>\n",
      "Epoch 1: train acc=0.8477, train loss=0.5032 | valid acc=0.8313, valid loss= 0.5293\n",
      "Epoch 2: train acc=0.8632, train loss=0.4660 | valid acc=0.8415, valid loss= 0.4959\n",
      "Epoch 3: train acc=0.8707, train loss=0.4456 | valid acc=0.8497, valid loss= 0.4785\n",
      "Epoch 4: train acc=0.8742, train loss=0.4368 | valid acc=0.8552, valid loss= 0.4727\n",
      "Epoch 5: train acc=0.8758, train loss=0.4339 | valid acc=0.8542, valid loss= 0.4718\n",
      "Epoch 6: train acc=0.8773, train loss=0.4310 | valid acc=0.8563, valid loss= 0.4705\n",
      "Epoch 7: train acc=0.8783, train loss=0.4282 | valid acc=0.8577, valid loss= 0.4694\n",
      "Epoch 8: train acc=0.8786, train loss=0.4273 | valid acc=0.8582, valid loss= 0.4700\n",
      "Epoch 9: train acc=0.8787, train loss=0.4269 | valid acc=0.8578, valid loss= 0.4710\n",
      "Epoch 10: train acc=0.8786, train loss=0.4278 | valid acc=0.8575, valid loss= 0.4731\n",
      "Epoch 11: train acc=0.8784, train loss=0.4294 | valid acc=0.8562, valid loss= 0.4757\n",
      "Epoch 12: train acc=0.8780, train loss=0.4309 | valid acc=0.8565, valid loss= 0.4782\n",
      "Epoch 13: train acc=0.8775, train loss=0.4326 | valid acc=0.8543, valid loss= 0.4806\n",
      "Epoch 14: train acc=0.8769, train loss=0.4333 | valid acc=0.8532, valid loss= 0.4818\n",
      "Epoch 15: train acc=0.8769, train loss=0.4335 | valid acc=0.8542, valid loss= 0.4826\n",
      "Epoch 16: train acc=0.8771, train loss=0.4335 | valid acc=0.8548, valid loss= 0.4831\n",
      "Epoch 17: train acc=0.8763, train loss=0.4353 | valid acc=0.8533, valid loss= 0.4854\n",
      "Epoch 18: train acc=0.8763, train loss=0.4357 | valid acc=0.8542, valid loss= 0.4859\n",
      "Epoch 19: train acc=0.8767, train loss=0.4348 | valid acc=0.8543, valid loss= 0.4853\n",
      "Epoch 20: train acc=0.8766, train loss=0.4356 | valid acc=0.8533, valid loss= 0.4860\n",
      "\n",
      "test acc=0.8524, test loss=0.5092 \n",
      "\n",
      "Parameter ['relu', 40, 40] (activation, n1 ,n2) has current best performance of test accuracy 0.8597\n"
     ]
    }
   ],
   "source": [
    "#Insert your code here. You can add more cells if necessary\n",
    "\n",
    "# The code with comment are used to figure out the best optimiser and the learning rate.\n",
    "# For saving time of teaching team to run and test the codes, \n",
    "# optimiser and learning rate are preset to Adam and 0.001 respectively \n",
    "\n",
    "# lst_opts = [tf.keras.optimizers.Adam(), tf.keras.optimizers.RMSprop(), tf.keras.optimizers.SGD(momentum=0.2)]\n",
    "# lst_learning_rates = [0.1, 0.01, 0.001]\n",
    "# opt = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "\n",
    "n1 = [20, 40]\n",
    "n2 = [20, 40]\n",
    "activation_choices = [\"relu\", \"sigmoid\", \"tanh\"]\n",
    "\n",
    "comb = ['', 0,0]\n",
    "highest_acc = 0.0\n",
    "\n",
    "# lr = 0.01\n",
    "# for p in lst_opts:\n",
    "#     for q in lst_learning_rates:\n",
    "#         for x in n1:\n",
    "#             for y in n2:\n",
    "#                 for z in activation_choices:\n",
    "#                     print(\"Current best parameters = \", comb)\n",
    "#                     p.learning_rate = q\n",
    "#                     dnn = DNN(epochs=5, activation_choice = z, n1 = x, n2 = y, optimizer=p, lr = q)\n",
    "#                     dnn.build()\n",
    "#                     dnn.train()\n",
    "#                     dnn.test()\n",
    "#                     if highest_acc <= dnn.test_acc():\n",
    "#                         highest_acc = dnn.test_acc\n",
    "#                         comb = [p,q,z,x,y]\n",
    "\n",
    "# print(comb)\n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "for x in n1:\n",
    "    for y in n2:\n",
    "        for z in activation_choices:\n",
    "            dnn = DNN(epochs=20, activation_choice = z, n1 = x, n2 = y, optimizer=opt, lr = 0.001)\n",
    "            dnn.build()\n",
    "            dnn.train()\n",
    "            dnn.test()\n",
    "            if highest_acc <= dnn.test_acc:\n",
    "                highest_acc = dnn.test_acc\n",
    "                comb = [z,x,y]\n",
    "            print()\n",
    "            print(\"Parameter {} (activation, n1 ,n2) has current best performance of test accuracy {}\".format(comb, highest_acc))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is activation function with relu, layer n1 of 40 and layer n2 of 40 with accuracy of 0.8597\n"
     ]
    }
   ],
   "source": [
    "print(\"The best model is activation function with {}, layer n1 of {} and layer n2 of {} with accuracy of {}\".format(comb[0], comb[1], comb[2], highest_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uYxJze1Kotx"
   },
   "source": [
    "####  <span style=\"color:red\">**Question 2.5**</span>. Experimenting with **sharpness-aware minimization** technique\n",
    "<div style=\"text-align: right\"> <span style=\"color:red\">[5 points]</span> </div>\n",
    "\n",
    "Sharpness-aware minimization (SAM) (i.e., [link for main paper](https://openreview.net/pdf?id=6Tm1mposlrM) from Google Deepmind) is a simple yet but efficient technique to improve the generalization ability of deep learning models on unseen data examples. In your research or your work, you might potentially use this idea. Your task is to read the paper and implement *Sharpness-aware minimization (SAM)*. Finally, you need to apply SAM to the best architecture found in **Question 2.4**.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9SSgSGuFKotx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================\n",
      "n1=40, n2=40, activation function=relu, learning rate=0.001, optimizer=<__main__.SAMOptimizer object at 0x000001FEC6141A90>\n",
      "Epoch 1: train acc=0.8196, train loss=0.5911 | valid acc=0.8158, valid loss= 0.5941\n",
      "Epoch 2: train acc=0.8425, train loss=0.5261 | valid acc=0.8340, valid loss= 0.5354\n",
      "Epoch 3: train acc=0.8530, train loss=0.4947 | valid acc=0.8443, valid loss= 0.5064\n",
      "Epoch 4: train acc=0.8590, train loss=0.4765 | valid acc=0.8502, valid loss= 0.4919\n",
      "Epoch 5: train acc=0.8629, train loss=0.4652 | valid acc=0.8555, valid loss= 0.4831\n",
      "Epoch 6: train acc=0.8648, train loss=0.4597 | valid acc=0.8560, valid loss= 0.4800\n",
      "Epoch 7: train acc=0.8664, train loss=0.4542 | valid acc=0.8558, valid loss= 0.4782\n",
      "Epoch 8: train acc=0.8691, train loss=0.4466 | valid acc=0.8557, valid loss= 0.4749\n",
      "Epoch 9: train acc=0.8693, train loss=0.4447 | valid acc=0.8553, valid loss= 0.4767\n",
      "Epoch 10: train acc=0.8716, train loss=0.4396 | valid acc=0.8572, valid loss= 0.4741\n",
      "Epoch 11: train acc=0.8717, train loss=0.4386 | valid acc=0.8568, valid loss= 0.4759\n",
      "Epoch 12: train acc=0.8730, train loss=0.4342 | valid acc=0.8567, valid loss= 0.4737\n",
      "Epoch 13: train acc=0.8754, train loss=0.4289 | valid acc=0.8572, valid loss= 0.4702\n",
      "Epoch 14: train acc=0.8752, train loss=0.4279 | valid acc=0.8587, valid loss= 0.4708\n",
      "Epoch 15: train acc=0.8767, train loss=0.4249 | valid acc=0.8593, valid loss= 0.4689\n",
      "Epoch 16: train acc=0.8758, train loss=0.4265 | valid acc=0.8568, valid loss= 0.4721\n",
      "Epoch 17: train acc=0.8772, train loss=0.4246 | valid acc=0.8567, valid loss= 0.4711\n",
      "Epoch 18: train acc=0.8766, train loss=0.4244 | valid acc=0.8568, valid loss= 0.4723\n",
      "Epoch 19: train acc=0.8775, train loss=0.4240 | valid acc=0.8582, valid loss= 0.4729\n",
      "Epoch 20: train acc=0.8762, train loss=0.4305 | valid acc=0.8565, valid loss= 0.4804\n",
      "\n",
      "test acc=0.8521, test loss=0.5079 \n"
     ]
    }
   ],
   "source": [
    "#Insert your code here. You can add more cells if necessary\n",
    "class SAMOptimizer(tf.keras.optimizers.Optimizer):\n",
    "    def __init__(self, base_optimizer, rho=0.05, name=\"SAMOptimizer\", **kwargs):\n",
    "        super(SAMOptimizer, self).__init__(name, **kwargs)\n",
    "        self.base_optimizer = base_optimizer\n",
    "        self.rho = rho\n",
    "\n",
    "    def _create_slots(self, var_list):\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, \"theta_old\")\n",
    "\n",
    "    def _resource_apply(self, grad, var, indices=None):\n",
    "        base_op = self.base_optimizer\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        theta_old = self.get_slot(var, \"theta_old\")\n",
    "\n",
    "        # Compute the sharpness-aware update\n",
    "        grad_norm = tf.norm(grad)\n",
    "        epsilon = tf.constant(1e-12, dtype=grad_norm.dtype)\n",
    "        coeff = self.rho / (grad_norm + epsilon)\n",
    "        delta_theta = coeff * grad\n",
    "\n",
    "        # Apply the sharpness-aware update to the variable\n",
    "        var_t = var - delta_theta\n",
    "\n",
    "        with tf.control_dependencies([base_op.apply_gradients([(var_t - var, var)])]):\n",
    "            # Store the current weights for the next iteration\n",
    "            theta_old.assign(var_t)\n",
    "\n",
    "        return tf.group(*[var.assign(var_t)])\n",
    "\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        return self._resource_apply(grad, var)\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var, indices=None):\n",
    "        raise NotImplementedError(\"Sparse gradient updates are not supported.\")\n",
    "        \n",
    "        \n",
    "base_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "opt = SAMOptimizer(base_optimizer, rho=0.05)\n",
    "\n",
    "dnn = DNN(optimizer=opt, epochs=20, batch_size=64, lr=0.001, activation_choice = 'relu', n1 = 40, n2 = 40)\n",
    "dnn.build()\n",
    "dnn.train()\n",
    "dnn.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6U4xpJfzKot3"
   },
   "source": [
    "---\n",
    "**<div style=\"text-align: center\"> <span style=\"color:black\">END OF PART 2</span> </div>**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
